{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Парадигма MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель вычислений. Алгоритмы могут быть реализованы с помощью парадигмы MapReduce (распределенные вычисления). Если на машине не хвататет памяти при вычислениях, то обращаются к распределенным вычислениям.\n",
    "\n",
    "2 шага:\n",
    "- `map`: обработка данных (применения функции ко всем элементам)\n",
    "- `reduce`: свертка данных (применение функции ко сбору всех элементов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/LE03/le03-MapReduce/le03-MapReduce-02.png\" title=\"Схема MapReduce\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Обработка происходит с помощью worker-ов:\n",
    "- Map phase == Mapper\n",
    "- Reduce phase == Reducer\n",
    "\n",
    "Файл или файлы разбиваются на разные части (split-ы). Каждый split может быть обработан своим worker-ом. ОБработка происходит параллельно и независимо. Worker-ы не могут общаться между собой. Worker-ы могут быть запущены не одновременно.\n",
    "\n",
    "Каждый mapper после обработки записывает себе данные на диск. \n",
    "\n",
    "После того, как все mapper-ы отработают происходит переход к reduce фазе.\n",
    "\n",
    "Данные reducer-у могут приходить от разных mapper-ов в формате **key -> value**. На repucer-е образуется этот **key** и массив всех значений, которые этому ключу соответствуют."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/LE03/le03-MapReduce/le03-MapReduce-03.png\" title=\"Входные данные\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Все данные разделяются на независимые split-ы и обрабатываются своим worker-ом. Если данные зависимы, то мы не сможем обработать такие связи, потому что их нет.\n",
    "\n",
    "Обычно, split==block в HDFS. Но, к примеру, храня файл архива `.gz` прослеживается явная зависимость блоков, так как по отдельности worker-ы не смогут разархивировать файл. В таком случае отправляем все блоки такого файла на один worker, хоть это и не оптимальное решение задачи.\n",
    "\n",
    "**Data locality** - стараются запускать worker ровно на той машине, на которой лежат данные. С диска читать быстрее, чем передавать по сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/LE03/le03-MapReduce/le03-MapReduce-04.png\" title=\"Передача данных между Map и Reduce\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Промежуточные данные mapper-ы хранят на локальных дисках, а не в HDFS, потому что в HDFS нужно производить репликацию данных. Мы теряем надежность данных, если произойдет ошибка. В таком случае MapReduce перезапускает такой mapper.\n",
    "\n",
    "**Shuffle** - процесс копирования с mapper-ов на reducer-ы. Обычно этот процесс запускается после того, как все mapper-ы завершились. Но можно и такую конфигурацию настроить, что как только mapper завершился, данные начинают копироваться на reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос**: как правильно разбивать файл на split-ы, чтобы одна и та же строка не оказалась в разных блоках?\n",
    "***Ответ**: лезем в реализацию класса `TextInputFormat`. Как это работает:\n",
    "- Есть функция, которая вычисляет сплиты. Она ничего не знает про концы строк. Она просто возвращает для каждого сплита название файла и сдвиг в нем. Сдвиг равен размеру блока. Т.е. обычно попадает на середину строки.\n",
    "- Есть функция, которая возвращает записи из данного сплита. Она берет файл, делает в нем нужный сдвиг, пропускает первую строку (т.е. делает `getLine()`) и начинает возвращать последовательно все следующие строки, пока текущая позиция не будет заходить за нужный сплит. При этом, **внимание!**, последняя строка (которая не вся влезла в этот сплит) считывается до конца, т.е. формально из другого сплита (и блока HDFS, если говорить про более низкий уровень). Именно поэтому, сначала первая строка пропускается, т.к. она уже попала в предыдущий сплит (конечно, для первого блока в файле не нужно пропускать строку). Для программы нет никаких сложностей считать строку, часть которой находится в одном блока, а часть в другой. Просто делаешь `getLine()` и получаешь нужную строку. На уровне HDFS при этом часть строки будет считано с одного блока (как правило локального), а остаток будет прислан по сети из другого блока."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/LE03/le03-MapReduce/le03-MapReduce-05.png\" title=\"Результат Map-Reduce задачи\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Разные reduce-ры (процессы) не могут писать в один и тот же файл. Выходные файлы хранятся в HDFS и реплицируются.\n",
    "\n",
    "Оптимально, когда каждый reducer обрабатывает от 500 мб до гб."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/LE03/le03-MapReduce/le03-MapReduce-06.png\" title=\"Map-Reduce без Reduce\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Частный случай задачи без Reduce фазы. Результат работы каждого mapper-а пишется сразу в output файл, которые хранятся в HDFS. \n",
    "\n",
    "Долгие задачи mapper-ов можно писать сразу  в HDFS, минуя локальное сохранение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**:  \n",
    "Число mapper-ов определяет система, а число reduce-ов в MapReduce задаче задается пользователем при описании MapReduce задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Фреймворк MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/LE03/le03-MapReduce/le03-MapReduce-09.png\" title=\"Схема работы демонов в  MapReduce\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "- Hadoop основан на HDFS. \n",
    "- На нашем кластере есть сервера (slave node) на которых запущены демоны datanode (непосредственно демоны файловой системы HDFS), которые работают поверх файловой системы Linux и обеспечивают работу с файловой системой HDFS.\n",
    "- Есть отдельный сервер, на котором запущен демон namenode. Он не хранит никаких данных, но отвечает за хранение мета-информации.\n",
    "- В MapReduce есть 2 типа демонов:\n",
    "    - Jobtracker - это процесс, который в целом отвечает за запуск задачи (job).\n",
    "    - Tasktracker запущен непосредственно на каждой машине кластера, на которой запущен datanode. Отвечает непосредственно за запуск worker-ов на node. Ничего не мешает запускать tasktracker и datanode на разных машинах, но тогда придется постоянно данные по сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JobTracker\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce/le03-MapReduce-10.png\" title=\"JobTracker\" width=\"500\" height=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TaskTracker\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce/le03-MapReduce-11.png\" title=\"TaskTracker\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Система слотов\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce/le03-MapReduce-12.png\" title=\"Система слотов\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Данная система слотов встречается в более ранних версиях Hadoop. В более новых - более новая, о ней дальше по курсу.\n",
    "\n",
    "Число слотов определяется из ресурсов сервера. Ресурсы сервера - это ЦПУ и Память оперативная. Обычно, каждый слот - одно ядро процессора и часть оперативной памяти.\n",
    "\n",
    "Слоты делятся на 2 типа: слоты для mapper-ов и слоты для reduce-ов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кейс\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce/le03-MapReduce-13.png\" title=\"Кейс\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Стоит задача: посчитать сколько раз встречается каждый символ в тексте.  \n",
    "Текст представляется в виде файлов `k1:v1, ... , k6:v6`. Так как у нас файл, то в качестве ключа номер строки, а в качестве значения сама строка. Далее каждый mapper считает сколько раз встретился каждый символ. Далее начинается фаза reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/LE03/le03-MapReduce/le03-MapReduce-14.png\" title=\"Кейс\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Перед началом фазы reduce мы должны каким-то образом скопировать данные из mapper-ов в reduce-ры. Этот этап называется **shuffle**. После этапа **shuffle** происходит этап **sort**.\n",
    "\n",
    "На один reduce-р попадают данные с одинаковым ключом от разных mapper-ов. Т.о. на одном reduce-ре мы для каждого ключа будем иметь все данные. \n",
    "\n",
    "Код на reduce-ре довольно простой. Мы просто сворачиваем в сумму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/LE03/le03-MapReduce/le03-MapReduce-15.png\" title=\"Кейс\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Схема может немного усложниться, если мы в середину добавим 2 этапа:\n",
    "- **combine** - это фактически локальный reduce-р. Делает свертку (агрегацию), но только для локальных данных. Обычно используют такой же код в combine, как и в reduce. К примеру, если на одном mapper-е у нас есть значения с одинаковым ключом, то мы их можем агрегировать.\n",
    "- **partition** определяет по ключу в какой reduce-р его нужно отправить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные функции\n",
    "\n",
    "- `map(k1,v1) -> list(k2, v2)`\n",
    "- `reduce(k2, list(v2*)) -> list(k3, v3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Опциональные функции\n",
    "\n",
    "- `partition(k2,v2, |reducers| ) -> № of reducer` \n",
    "    - распределяет ключи по reducer-ам\n",
    "    - часто просто хеш от key: `hash(k2) mod n`\n",
    "- `combine(k2, v2) -> list(k2,v2)`\n",
    "    - мини-reducer-ы, которые выполняются после завершения фазы map\n",
    "    - используется в качестве оптимизации для снижения сетевого трафика на reduce\n",
    "    - (!) не должен менять тип ключа и значения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Java API. Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Типы API\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-02.png\" title=\"Типы API.\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класс Job\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-03.png\" title=\"Класс Job\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класс Mapper\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-04.png\" title=\"Класс Mapper\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "В отличии от класса job, который создает объект, класс mapper должен наследоваться в пользовательском классе. Плюс, в пользовательском классе должны быть реализованы некоторые функции: \n",
    "- `map`, которая будет вызываться для каждого из split-а. Так, же будет передан `context`, из которого можно получить информацию о всей задаче. А так же, может передавать некоторую статистику в Jobtracker.\n",
    "- `setup` запускается, чтобы что-то проинициализировать.\n",
    "- `cleanup` зеркальная `setup` запускается в конце работы mapper-а и закрывает те ресурсы, которые были выделены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класс Reducer/Combiner\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-05.png\" title=\"Класс Reducer/Combiner\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "По аналогии с классом Mapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класс Partitioner\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-06.png\" title=\"Класс Partitioner\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Java API. Продолжение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer в качестве Combiner\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-19.png\" title=\"Reducer в качестве Combiner\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "В общем случае, если каку-то операцию можно делать частично (сумму от части данных, а потом сумму от итогового результата), то тогда combiner применять можно, иначе применять нельзя.\n",
    "\n",
    "MapReduce **не гарантирует** вызов combiner-а, он служит только для некоторой оптимизации, чтобы данных передавалось меньше между mapper-ом и reducer-ом. Поэтому нет смысла закладывать некую логику внего, тк он может быть и не вызван."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Типы данных в Hadoop\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-20.png\" title=\"Типы данных в Hadoop\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Для передачи данных между mapper-ом и reducer-ом не используются стандартные типы данных, такие как int, flor или строки. Для этого внутренние типы должны иметь внутренние методы для сериализации и десериализации данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класс InputSplit\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-23.png\" title=\"Класс InputSplit\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класс InputFormat\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-24.png\" title=\"Класс InputFormat\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-25.png\" title=\"Готовые классы InputFormat\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класс OutputFormat\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-26.png\" title=\"Класс OutputFormat\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-27.png\" title=\"Класс OutputFormat\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle и Sort в Hadoop\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-29.png\" title=\"Shuffle и Sort в Hadoop\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Важно понимать, как происходит процесс передачи данных от mapper-а к reducer-у. При большом объему данных время, затраченное на передачу данных, существенно. И нужно понимать на какие процессы уходит время.\n",
    "\n",
    "Те данные, которые которые пишет mapper попадает в **цикличный буффер (circ buff)**. Размер его ограничен, он задается в файле конфигурации. Когда буффер заполняется он сбрасывает их в файлы spill-ы на диске. По окончанию работы mapper-а таких файлов может оказаться много и мы должны объединить их в один файл. Все эти файлы зачитываются, сортируются и merg-атся в один файл. Причем может работать combiner (локальный reducer), который агрегирует непосредственно на mapper-е. При подготовке итогового файла (он не просто монолитный, он разбит на разные части) каждая часть соответствует определенному reducer-у. То каким образом разбивается файл определяется partition-ом. Каждая часть этого файла будет отсортирована (это довольно важно для дальнейшей работы reducer-а). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-30.png\" title=\"Shuffle и Sort в Hadoop\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "После того, как на каждом mapper-е образовался файл, разбитый на части. Каждая его часть будет скопирована на соответствующий reducer. При этом другие части пойдут на другие reducer-ы. Этот процесс копирования файлов называется **shuffle**. На одном конкретном reducer-е мы получаем много частей файлов с разных mapper-ов. Далее производим так называемый процесс сортировки. Мы помним, что данные в каждой части уже как-то отсортированы. Задача упрощается и мы получаем отсортированный поток, который отправляется в reducer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск и отладка MapReduce программы\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-31.png\" title=\"Запуск MapReduce программы\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-32.png\" title=\"Отладка MapReduce программы\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Streaming\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-34.png\" title=\"Hadoop Streaming\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Hadoop streaming - инструмент для запуска команд на другом языке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map: `countMap.py`\n",
    "```python\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for token in line.strip().split(\" \"):\n",
    "        if token: print(token + \"\\t1\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce: `countReduce.py`\n",
    "```python\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "(lastKey, sum) = (None, 0)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    (key, value) = line.strip().split(\"\\t\")\n",
    "    if lastKey and lastKey != key:\n",
    "        print(lastKey + \"\\t\" + str(sum))\n",
    "        (lastKey, sum) = (key, int(value))\n",
    "    else:\n",
    "        (lastKey, sum) = (key, sum + int(value))\n",
    "        \n",
    "if lastKey:\n",
    "    print(lastKey + \"\\t\" + str(sum))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск и отладка\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-38.png\" title=\"Запуск и отладка\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Сначала запускаем и проверяем корректность работы локально.\n",
    "\n",
    "<img src=\"slides/LE03/le03-MapReduce_API/le03-MapReduce_API-39.png\" title=\"Запуск и отладка\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Запускаем с помощью Hadoop streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
